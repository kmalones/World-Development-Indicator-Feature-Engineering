# -*- coding: utf-8 -*-
"""Homework2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkBHtsMfPRhgy_Y_-f7Bs8upWbxf_sAQ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, accuracy_score

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/BUDT704/Exercises/LDatasets/worlddata.csv')

df.head(5)

# im doing this to see what has the most data.
START_YEAR = 2000
END_YEAR = 2015

df_filtered = df[(df['Year'] >= START_YEAR) & (df['Year'] <= END_YEAR)].copy()
df_complete = df_filtered.dropna(subset=['Value'])

indicator_counts = df_complete.groupby('IndicatorName')['Value'].count().reset_index()
indicator_counts.rename(columns={'Value': f'Total_Non_Missing_Observations_({START_YEAR}-{END_YEAR})'}, inplace=True)

most_complete_indicators = indicator_counts.sort_values(
    by=f'Total_Non_Missing_Observations_({START_YEAR}-{END_YEAR})',
    ascending=False
)
print(f"\nTop 15 Most Complete Indicators (Years {START_YEAR} to {END_YEAR}):")
print(most_complete_indicators.head(30).to_markdown(index=False))

chosen_indicators = [
    'Population, total',
    'Internet users (per 100 people)',
    'Mortality rate, infant (per 1,000 live births)',
    'Population density (people per sq. km of land area)',
    'Mobile cellular subscriptions (per 100 people)'
]

df_filtered = df[(df['Year'] >= START_YEAR) & (df['Year'] <= END_YEAR)].copy()
df_filtered_chosen = df_filtered[df_filtered['IndicatorName'].isin(chosen_indicators)]

wide_df = df_filtered_chosen.pivot_table(
    index=['CountryName', 'Year', 'CountryCode'],
    columns='IndicatorName',
    values='Value'
).reset_index()

wide_df.columns = wide_df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
wide_df.rename(columns={
    'Population_total': 'Population_Total',
    'Internet_users_per_100_people_': 'Internet_Users',
    'Mortality_rate_infant_per_1_000_live_births_': 'Infant_Mortality',
    'Population_density_people_per_sq__km_of_land_area_': 'Population_Density',
    'Mobile_cellular_subscriptions_per_100_people_': 'Mobile_Subs'
}, inplace=True)

pop_median = wide_df['Population_Total'].median()

wide_df['Is_High_Pop'] = np.where(wide_df['Population_Total'] > pop_median, 1, 0)
wide_df.drop(columns=['Population_Total'], inplace=True)

wide_df_clean = wide_df.dropna()
wide_df_clean.to_csv('worlddata_clean_wide.csv', index=False)

print("\n--- Data Preparation Complete ---")
print(f"Final dataset shape after cleaning: {wide_df_clean.shape}")
print(f"Count of High-Population observations (1): {wide_df_clean['Is_High_Pop'].sum()}")
print("Head of the clean dataset:")
print(wide_df_clean.head().to_markdown(index=False))

"""Now I'm going to start classification modeling"""

def cumulativeGainChart(pred_y, actual_y, title='Cumulative Gain Chart', figsize=(8, 8)):

    df = pd.DataFrame({'actual': actual_y, 'predicted': pred_y})
    df = df.sort_values('predicted', ascending=False)


    df['cumulative_actual'] = df['actual'].cumsum()
    total_positives = df['actual'].sum()
    df['gain'] = df['cumulative_actual'] / total_positives
    df['percent_obs'] = (np.arange(len(df)) + 1) / len(df)


    plt.figure(figsize=figsize)
    plt.plot(df['percent_obs'], df['gain'], label='Model Gain', color='red')
    plt.plot([0, 1], [0, 1], linestyle='--', label='Random Model', color='grey')
    plt.xlabel('Percentage of Observations Targeted (Sorted by Model Probability)')
    plt.ylabel('Cumulative Percentage of High-Pop Countries Captured')
    plt.title(title)
    plt.legend()
    plt.grid(True)

FEATURES = ['Internet_Users', 'Mobile_Subs', 'Infant_Mortality', 'Population_density_people_per_sq_km_of_land_area_']
TARGET = 'Is_High_Pop'

X = wide_df_clean[FEATURES]
y = wide_df_clean[TARGET]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)
print(f"Training observations: {len(X_train)}")
print(f"Validation observations: {len(X_val)}")

model = LogisticRegression(solver='liblinear', random_state=42)
model.fit(X_train, y_train)

pred_prob = model.predict_proba(X_val)[:, 1]

pred_class = model.predict(X_val)

roc_auc = roc_auc_score(y_val, pred_prob)
print(f"\nArea Under the ROC Curve (AUC): {roc_auc:.4f}")


fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.close()

cumulativeGainChart(pred_prob, y_val, title='Cumulative Gain Chart: Predicting High-Pop Countries')

cm = confusion_matrix(y_val, pred_class)
accuracy = accuracy_score(y_val, pred_class)

print("\n--- Model Evaluation (Validation Set) ---")
print("Confusion Matrix:\n", cm)
print(f"Accuracy: {accuracy:.4f}")
print("Classification complete. Check your directory for the 'roc_curve.png' and 'cumulative_gain_chart.png' files.")

coefficients = pd.DataFrame({
    'Feature': FEATURES,
    'Coefficient': model.coef_[0]
})

coefficients['Odds_Ratio'] = np.exp(coefficients['Coefficient'])

print("\nLogistic Regression Coefficients (for Interpretation):")
print(coefficients.to_markdown(index=False))

print("--- 1. Receiver Operating Characteristic (ROC) Curve ---")
display(Image(filename='roc_curve.png'))


print("\n--- 2. Cumulative Gain Chart ---")
display(Image(filename='cumulative_gain_chart.png'))